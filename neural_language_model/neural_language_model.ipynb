{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process \n",
    "\n",
    "* get train and test data \n",
    "* create tokenizer from data \n",
    "* create dataset \n",
    "* create dataloader \n",
    "* load model \n",
    "* training \n",
    "* save model\n",
    "* testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from csv_dataset import CSV_Dataset\n",
    "from tokenizer import Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from model import SimpleNN\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CSV_Dataset(train_data)\n",
    "test_ds = CSV_Dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_ds, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(type(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['in the executive board of the state railways wanted to unk the operation of restaurant carriages and the most important station restaurants in norway under one management the board stated that they wanted to minimise the conflict of interest between the railway company and the dining car operator they also saw unk operations as a way to unk a larger share of the revenue to the railway company and to ensure a high quality of service on new lines at that time the unk line and dovre line were in the planning stages and the nsb intended to introduce dining services on these when they opened oslo east station and its restaurant were operated by the private norwegian trunk railway in an agreement signed on september both railway companies agreed that a new restaurant operator would be controlled by the norwegian trunk railway but this company had to abide by the nsb s decision of how many restaurant carriages to operate on any line',\n",
       " 'razor ray guitar soloist vocals background vocals',\n",
       " 'release and reception',\n",
       " 'at this point for a period which coincided with the birth and childhood of his natural son unk miriam delaborde alkan withdrew into private study and composition for six years returning to the concert platform only in alkan neither asserted or denied his paternity of delaborde which however his contemporaries seemed to assume marmontel wrote unk in a biography of delaborde that his birth is a page from a novel in the life of a great artist alkan gave early piano lessons to delaborde who was to follow his natural father as a keyboard virtuoso',\n",
       " 'hornung turned away from raffles thereafter and in february published the camera unk a thriller whose narrator is an unk cricket enthusiast with an unk father much as hornung was himself the story concerned the attempts of a scientist to photograph the soul as it left the body hornung followed this up with fathers of men and the thousandth woman before unk hill a collection of eight short stories in which he introduced the characters unk unk and the narrator unk whom rowland considers to be unk of raffles and bunny hornung s next work the crime doctor marked the end of his fictional output',\n",
       " 'alice in chains has also had a significant influence on modern heavy metal their songs were covered by various metal bands such as unk dream theater unk of the moon suicide silence and grave unk and unk guitarist unk unk had expressed his admiration for jerry cantrell s guitar work in an interview for guitar international saying that the unk and the honest feel that jerry cantrell gets on alice in chains dirt record is worth a lot more than someone who plays five million notes anders unk of swedish melodic death metal band in flames cited layne staley as an inspiration for his vocals on the band s later albums in addition to fellow musicians the band has also received praise from critics with steve huey of allmusic calling them one of the best metal bands of the s upon reviewing the compilation nothing safe',\n",
       " 'uss mcdougal destroyer no dd was laid down by bath iron works of bath maine in july and launched in april the ship was the second u s navy vessel named in honor of david stockton mcdougal a u s navy officer notable for his leadership during an battle off japan while in command of wyoming',\n",
       " 'thunderbird a space station that relays distress calls from around the world unk alternately by space unk john and alan']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(batch))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "training_config = {\"vocab_size\": tokenizer.get_vocab_size(), \n",
    "                \"embedding_dim\": 100, \n",
    "                \"hidden_dim\": 512, \n",
    "                \"learning_rate\": 1e-3,\n",
    "                \"epochs\": 30,\n",
    "                \"batch_size\": 8,\n",
    "                \"window_size\": 5,\n",
    "                \"device\": device}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(vocab_size=training_config[\"vocab_size\"], \n",
    "                 embedding_dim=training_config[\"embedding_dim\"], \n",
    "                 hidden_dim=training_config[\"hidden_dim\"], \n",
    "                 window_size=training_config[\"window_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Loss_func():\n",
    "    return torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def generate_data(batch: list[str], tokenizer= None, config: dict= None):\n",
    "    data = tokenizer.encode(batch)\n",
    "    inputs = []\n",
    "    targets = [] \n",
    "    for sample in data:\n",
    "        for idx in range(len(sample) - config[\"window_size\"]):\n",
    "            inputs.append(sample[idx:idx+config[\"window_size\"]])\n",
    "            targets.append(sample[idx+config[\"window_size\"]])\n",
    "    return torch.tensor(inputs, dtype= torch.LongStorage), torch.tensor(targets, dtype= torch.long)\n",
    "\n",
    "def model_step(batch, model, loss_func, config: dict):\n",
    "    _input, target = batch\n",
    "    _input = _input.to(config['device'])\n",
    "    target = target.to(config['device'])\n",
    "    outputs = model(_input)\n",
    "    loss = loss_func(outputs, target)\n",
    "    \n",
    "    del _input, target, outputs\n",
    "    return loss\n",
    "\n",
    "def optimizer_step(optimizer, loss):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def train_epoch(model, train_loader, loss_func, optimizer, tokenizer, config: dict, epoch):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    total, total_loss = 0, 0\n",
    "    for batch in pbar:\n",
    "        inputs, targets = generate_data(batch, tokenizer, config)\n",
    "        for idx in range(0, len(inputs) , config[\"batch_size\"]):\n",
    "            x = inputs[idx:idx+config[\"batch_size\"]]\n",
    "            y = targets[idx:idx+config[\"batch_size\"]]\n",
    "            loss= model_step((x, y), model, loss_func, config= config)\n",
    "            optimizer_step(optimizer, loss)\n",
    "\n",
    "            total += len(y)\n",
    "            total_loss += loss.item()\n",
    "        pbar.set_description(f\"epoch = {epoch}, train/loss = {total_loss/total:.3f}\")\n",
    "            \n",
    "    return (total_loss / total)    #loss\n",
    "\n",
    "\n",
    "def plot_training_history(train_loss_history):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "def train(model, train_loader, tokenizer = None, plot_res = True, config: dict = None):\n",
    "    train_loss_history = []\n",
    "    loss_func = get_Loss_func()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), config[\"learning_rate\"])\n",
    "    model.to(config['device'])\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        ## train loop\n",
    "        train_loss = train_epoch(model, train_loader, loss_func, optimizer, tokenizer, config, epoch)\n",
    "        train_loss_history.append(train_loss)\n",
    "            \n",
    "    if(plot_res):\n",
    "        plot_training_history(train_loss_history)\n",
    "    return model, train_loss_history\n",
    "\n",
    "def save_weight(model, output_file):\n",
    "    print(\"save model to\", output_file)\n",
    "    torch.save(model.state_dict(), output_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (embedding): Embedding(27159, 100)\n",
      "  (relu1): LeakyReLU(negative_slope=0.15)\n",
      "  (flaten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=500, out_features=512, bias=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.15)\n",
      "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (relu3): LeakyReLU(negative_slope=0.15)\n",
      "  (fc3): Linear(in_features=512, out_features=27159, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(training_config['learning_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17167635"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate total params \n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch = 0, train/loss = 1.746:   3%|▎         | 86/3067 [00:57<32:57,  1.51it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 61\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, tokenizer, plot_res, config)\u001b[0m\n\u001b[0;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mto(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m## train loop\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     train_loss_history\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(plot_res):\n",
      "Cell \u001b[1;32mIn[35], line 39\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, loss_func, optimizer, tokenizer, config, epoch)\u001b[0m\n\u001b[0;32m     36\u001b[0m         optimizer_step(optimizer, loss)\n\u001b[0;32m     38\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m---> 39\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train/loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (total_loss \u001b[38;5;241m/\u001b[39m total)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, training_history = train(model = model, train_loader= train_loader, tokenizer = tokenizer, config= training_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, tokenizer, config):\n",
    "    loss_func = get_Loss_func()\n",
    "    model.eval()\n",
    "    total, total_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = generate_data(batch, tokenizer, config)\n",
    "        for idx in range(0, len(inputs) , config[\"batch_size\"]):\n",
    "            x = inputs[idx:idx+config[\"batch_size\"]]\n",
    "            y = targets[idx:idx+config[\"batch_size\"]]\n",
    "            loss= model_step((x, y), model, loss_func, config= config)\n",
    "            \n",
    "            total += 1\n",
    "            total_loss += loss.item()\n",
    "    res_loss = total_loss / total\n",
    "    perplexity = np.exp(res_loss)\n",
    "    return perplexity, res_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net: nn.Module, tokenizer, text: str, window_size: int):\n",
    "    text_id = tokenizer.encode(text)\n",
    "    generated_text = [] \n",
    "    if len(text_id) < window_size:\n",
    "        text_id = [[tokenizer.encode('<pad>')]] * (window_size - len(text_id)) + text_id\n",
    "    for i in range(100):\n",
    "        inputs = text_id + generated_text\n",
    "        inputs = inputs[-window_size:]\n",
    "        inputs = torch.tensor(inputs, dtype= torch.long, device= training_config['device']).unsqueeze(0)\n",
    "        prob = net(inputs)\n",
    "        prob = torch.softmax(prob, dim=1)\n",
    "        next_word_id = torch.argmax(prob).item()\n",
    "        generated_text.append(next_word_id)\n",
    "        \n",
    "    return tokenizer.decode(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
